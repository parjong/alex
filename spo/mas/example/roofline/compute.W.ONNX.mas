# What is this .mas script for
#
# This script computes MAC (Multiply-accumulate) operation count
import logging
from logging import warn

def ignored(reason):
  def f(l):
    global logging
    logging.debug(f'OPS from {l.ind}:{l.op} is ignored (reason)')
    return 0
  # def f: END
  return f
# def ignored: END


# Use this helper for operations without MAC operation (e.g. CONCAT, TRANSPOSE)
def out_of_scope():
  global ignored
  return ignored('no MAC operation')
# def out_of_scope: END


def unimplemented():
  def f(l):
    global warn
    warn(f'OPS from {l.ind}:{l.op} is ignored (not implemented, yet)')
    return 0
  # def f: END
  return f
# def unimplemented: END


# from ONNX 11
#
# See https://github.com/onnx/onnx/blob/main/docs/Operators.md#conv
#
# Changes(?)
# - All input/outputs are marked as 'differentiable'
#
# from ONNX 1
#
# https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Conv-1
def conv(version):
  def R1(l):
    assert l.input_count >= 2
    assert l.input_count <= 3
    ifm_value = l.input(0)
    wgt_value = l.input(1)

    assert l.output_count == 1
    out_value = l.output(0)

    # NCHW
    assert ifm_value.rank == 4
    IFM_C = ifm_value.dim(1)

    # NCHW
    assert out_value.rank == 4

    # Group convolution is not yet supported
    assert l.attr('group') == 1
    FLT_H, FLT_W = l.attr('kernel_shape')

    OPS_PER_OUT_ELEM = 0
    # For each IFM/FLT pixel pair:
    #   One multiplication
    #   One addition (for accumulation)
    OPS_PER_OUT_ELEM += 2 * FLT_H * FLT_W * IFM_C
    # Add a bias
    OPS_PER_OUT_ELEM += 1 if l.input_count == 3 else 0

    return OPS_PER_OUT_ELEM * out_value.elem_count
  # def R1: END
  return R1
# def conv: END

def tconv2d(l):
  warn(f'OPS from layer {l.ind} ({l.op}) is not verified, yet')
  # Input 0 => output_shape
  # Input 1 => weights
  # Input 2 => input
  assert l.input_count == 3
  assert l.output_count == 1
  inp_value = l.input(2)
  wgt_value = l.input(1)
  out_value = l.output(0)

  assert inp_value.rank == 4
  assert out_value.rank == 4

  # WGT is [O, H, W, I]
  assert inp_value.dim(3) == wgt_value.dim(3)
  assert out_value.dim(3) == wgt_value.dim(0)
  FLT_H = wgt_value.dim(1)
  FLT_W = wgt_value.dim(2)

  OUT_C = out_value.dim(3)

  OPS_PER_INP_ELEM = 0
  # For each IFM pixel:
  #   For each OFM/FLT pixel pair:
  #     One multiplication (IFM * FLT)
  #     One addition (for accumulation)
  OPS_PER_INP_ELEM += 2 * FLT_H * FLT_W * OUT_C
  # tfl.TRANSPOSE_CONV has no bias

  return OPS_PER_INP_ELEM * inp_value.elem_count

def eltwise_binary(l):
  assert l.output_count == 1
  out_value = l.output(0)

  assert l.input_count == 2

  # Run
  #
  # acc = op.identity_element (0 for ADD / 1 for MUL)
  # for each input I:
  #   acc = op(acc, LHS)
  #   acc = op(acc, RHS)
  #
  OPS_PER_OUT_ELEM = 2

  return OPS_PER_OUT_ELEM * out_value.elem_count

def reduce_mean(l):
  assert l.input_count >= 1
  assert l.output_count == 1

  in_value = l.input(0)
  out_value = l.output(0)

  assert in_value.elem_count >= out_value.elem_count

  # N input elements are reduced as one output element
  N = in_value.elem_count / out_value.elem_count

  OPS_PER_OUT_ELEM = 0
  # Accumulate N elemets
  OPS_PER_OUT_ELEM += N
  # One division
  OPS_PER_OUT_ELEM += 1

  return OPS_PER_OUT_ELEM * out_value.elem_count

def fully_connected(l):
  assert l.input_count in [2, 3]
  assert l.output_count == 1
  inp_value = l.input(0)
  out_value = l.output(0)

  assert inp_value.dim(0) == out_value.dim(0)

  # Q. Is this correct??
  B = inp_value.dim(0)
  INP_ELEM = inp_value.elem_count / B
  OUT_ELEM = out_value.elem_count / B

  OPS_PER_OUT_ELEM = 0
  OPS_PER_OUT_ELEM += 2 * INP_ELEM
  OPS_PER_OUT_ELEM += 1 if l.input_count == 3 else 0

  # for each BATCH
  #   for each OUT
  #     acc = 0
  #     for each INP
  #       acc += INP * WGT
  #     acc += BIAS
  #
  # TODO Revisit this definition
  return B * OUT_ELEM * OPS_PER_OUT_ELEM

def analyze(l):
  print(l.attr_keys)
  exit(1)
  return

# How to compute OPS per each layer
# OPNAME -> HELPER
rules = { }

rules['ai.onnx.Add'] = eltwise_binary
rules['ai.onnx.AveragePool'] = out_of_scope()
rules['ai.onnx.BatchNormalization'] = ignored('expect to be fused')
rules['ai.onnx.Concat'] = out_of_scope()
# ONNX v9
rules['ai.onnx.Conv'] = conv(1)
rules['ai.onnx.Flatten'] = out_of_scope()
rules['ai.onnx.Gemm'] = fully_connected
rules['ai.onnx.GlobalAveragePool'] = out_of_scope()
rules['ai.onnx.MaxPool'] = out_of_scope()
rules['ai.onnx.Neg'] = out_of_scope() # Q. Is this true?
rules['ai.onnx.Pad'] = out_of_scope()
rules['ai.onnx.Relu'] = out_of_scope()
rules['ai.onnx.Squeeze'] = out_of_scope()

# Analyze model
mas = get_mas_session()
assert mas.model.opset.startswith('onnx')

# Iterate each layer
MODEL_OPS = 0

for layer in mas.model.layers:
  compute_ops = rules[layer.op]
  LAYER_OPS = compute_ops(layer)
  MODEL_OPS += LAYER_OPS
# for: END

logging.info(f'OPS count is {MODEL_OPS}')
mas.set_attr('roofline.W.model', MODEL_OPS)
